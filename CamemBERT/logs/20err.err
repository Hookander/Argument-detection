wandb: Currently logged in as: balthazar-martin123. Use `wandb login --relogin` to force relogin
wandb: Agent Starting Run: 7ndmt9jc with config:
wandb: 	batch_size: 8
wandb: 	lr: 8.973390679384108e-05
wandb: 	model_name: camembert-base
wandb: 	patience: 15
wandb: 	weight_decay: 0.07125891848456185
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_193542-7ndmt9jc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-1
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/7ndmt9jc
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-SXM4-80GB MIG 2g.20gb') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                               | Params
-------------------------------------------------------------
0 | model | CamembertForSequenceClassification | 110 M 
-------------------------------------------------------------
110 M     Trainable params
0         Non-trainable params
110 M     Total params
442.497   Total estimated model params size (MB)
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
slurmstepd: error: *** JOB 7548 ON dgxa100 CANCELLED AT 2024-04-06T19:36:01 ***
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:             test/f1 â–
wandb:          train/loss â–‡â–ƒâ–ƒâ–…â–…â–„â–ƒâ–„â–†â–ƒâ–…â–â–ƒâ–‚â–â–â–â–…â–ƒâ–â–ƒâ–ˆâ–…â–ƒâ–‡â–‡â–…â–ƒâ–â–ƒâ–ƒâ–„â–â–‡â–…â–…â–â–„â–ƒâ–ƒ
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:           valid/acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            valid/f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 16
wandb:            test/acc 0.5
wandb:             test/f1 0.33333
wandb:          train/loss 0.4351
wandb: trainer/global_step 2768
wandb:           valid/acc 0.85797
wandb:            valid/f1 0.69942
wandb: 
wandb: ğŸš€ View run kind-sweep-1 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/7ndmt9jc
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_193542-7ndmt9jc/logs
wandb: Agent Starting Run: wtb9rn4t with config:
wandb: 	batch_size: 32
wandb: 	lr: 0.0002633215096514913
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 15
wandb: 	weight_decay: 0.07545740189202399
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_194900-wtb9rn4t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-2
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/wtb9rn4t
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                               | Params
-------------------------------------------------------------
0 | model | CamembertForSequenceClassification | 336 M 
-------------------------------------------------------------
336 M     Trainable params
0         Non-trainable params
336 M     Total params
1,346.658 Total estimated model params size (MB)
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (44) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.025 MB uploadedwandb: | 0.015 MB of 0.025 MB uploadedwandb: / 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:             test/f1 â–
wandb:          train/loss â–†â–„â–‡â–„â–â–„â–ˆâ–†â–„â–„â–†â–‚â–„â–ƒ
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:           valid/acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            valid/f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 16
wandb:            test/acc 0.5
wandb:             test/f1 0.33333
wandb:          train/loss 0.39258
wandb: trainer/global_step 704
wandb:           valid/acc 0.85797
wandb:            valid/f1 0.55076
wandb: 
wandb: ğŸš€ View run zesty-sweep-2 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/wtb9rn4t
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_194900-wtb9rn4t/logs
wandb: Agent Starting Run: rfcdbfmu with config:
wandb: 	batch_size: 16
wandb: 	lr: 0.0008014625433201173
wandb: 	model_name: camembert-base
wandb: 	patience: 10
wandb: 	weight_decay: 0.08112480669823825
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_202854-rfcdbfmu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-3
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/rfcdbfmu
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                               | Params
-------------------------------------------------------------
0 | model | CamembertForSequenceClassification | 110 M 
-------------------------------------------------------------
110 M     Trainable params
0         Non-trainable params
110 M     Total params
442.497   Total estimated model params size (MB)
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:            test/acc â–
wandb:             test/f1 â–
wandb:          train/loss â–†â–ƒâ–…â–…â–„â–„â–ˆâ–…â–„â–…â–â–ƒâ–‚â–…â–„â–…â–‚â–†â–…
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:           valid/acc â–â–â–â–â–â–â–â–â–â–â–
wandb:            valid/f1 â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 11
wandb:            test/acc 0.5
wandb:             test/f1 0.33333
wandb:          train/loss 0.60873
wandb: trainer/global_step 957
wandb:           valid/acc 0.85797
wandb:            valid/f1 0.61618
wandb: 
wandb: ğŸš€ View run misunderstood-sweep-3 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/rfcdbfmu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_202854-rfcdbfmu/logs
wandb: Agent Starting Run: sl8tqd55 with config:
wandb: 	batch_size: 16
wandb: 	lr: 0.0002924788728510897
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 5
wandb: 	weight_decay: 0.08769219502303215
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_203732-sl8tqd55
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-4
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/sl8tqd55
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                               | Params
-------------------------------------------------------------
0 | model | CamembertForSequenceClassification | 336 M 
-------------------------------------------------------------
336 M     Trainable params
0         Non-trainable params
336 M     Total params
1,346.658 Total estimated model params size (MB)
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆ
wandb:            test/acc â–
wandb:             test/f1 â–
wandb:          train/loss â–â–ˆâ–ƒâ–„â–‚â–‚â–‚â–‚â–„â–†
wandb: trainer/global_step â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:           valid/acc â–â–â–â–â–â–
wandb:            valid/f1 â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 6
wandb:            test/acc 0.5
wandb:             test/f1 0.33333
wandb:          train/loss 0.76306
wandb: trainer/global_step 522
wandb:           valid/acc 0.85797
wandb:            valid/f1 0.61618
wandb: 
wandb: ğŸš€ View run electric-sweep-4 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/sl8tqd55
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_203732-sl8tqd55/logs
wandb: Agent Starting Run: h6tf9i5h with config:
wandb: 	batch_size: 64
wandb: 	lr: 3.148673356370458e-05
wandb: 	model_name: camembert-base
wandb: 	patience: 5
wandb: 	weight_decay: 0.01373485832136966
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_205312-h6tf9i5h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-5
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/h6tf9i5h
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                               | Params
-------------------------------------------------------------
0 | model | CamembertForSequenceClassification | 110 M 
-------------------------------------------------------------
110 M     Trainable params
0         Non-trainable params
110 M     Total params
442.497   Total estimated model params size (MB)
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.023 MB uploadedwandb: | 0.015 MB of 0.023 MB uploadedwandb: / 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–‚â–ƒâ–ƒâ–…â–†â–†â–‡â–ˆ
wandb:            test/acc â–
wandb:             test/f1 â–
wandb:          train/loss â–â–ˆ
wandb: trainer/global_step â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–ˆ
wandb:           valid/acc â–â–â–â–â–â–
wandb:            valid/f1 â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 6
wandb:            test/acc 0.5
wandb:             test/f1 0.33333
wandb:          train/loss 0.50851
wandb: trainer/global_step 132
wandb:           valid/acc 0.85797
wandb:            valid/f1 0.41468
wandb: 
wandb: ğŸš€ View run glorious-sweep-5 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/h6tf9i5h
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_205312-h6tf9i5h/logs
wandb: Agent Starting Run: c8z28mcu with config:
wandb: 	batch_size: 64
wandb: 	lr: 0.0007131144191386862
wandb: 	model_name: camembert-base
wandb: 	patience: 30
wandb: 	weight_decay: 0.0018368185927849724
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_205745-c8z28mcu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-6
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/c8z28mcu
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                               | Params
-------------------------------------------------------------
0 | model | CamembertForSequenceClassification | 110 M 
-------------------------------------------------------------
110 M     Trainable params
0         Non-trainable params
110 M     Total params
442.497   Total estimated model params size (MB)
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.026 MB uploadedwandb: | 0.015 MB of 0.026 MB uploadedwandb: / 0.026 MB of 0.026 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:             test/f1 â–
wandb:          train/loss â–‡â–…â–ˆâ–â–â–ƒâ–†â–†â–‡â–‡â–†â–‚â–‡
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:           valid/acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            valid/f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 31
wandb:            test/acc 0.5
wandb:             test/f1 0.33333
wandb:          train/loss 0.58141
wandb: trainer/global_step 682
wandb:           valid/acc 0.85797
wandb:            valid/f1 0.41468
wandb: 
wandb: ğŸš€ View run legendary-sweep-6 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/c8z28mcu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_205745-c8z28mcu/logs
wandb: Agent Starting Run: jievzz6h with config:
wandb: 	batch_size: 16
wandb: 	lr: 7.236820342589106e-05
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 15
wandb: 	weight_decay: 0.05670918330117555
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_212021-jievzz6h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-7
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/jievzz6h
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                               | Params
-------------------------------------------------------------
0 | model | CamembertForSequenceClassification | 336 M 
-------------------------------------------------------------
336 M     Trainable params
0         Non-trainable params
336 M     Total params
1,346.658 Total estimated model params size (MB)
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.025 MB uploadedwandb: | 0.015 MB of 0.025 MB uploadedwandb: / 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:            test/acc â–
wandb:             test/f1 â–
wandb:          train/loss â–„â–‚â–…â–„â–‚â–‡â–†â–…â–ƒâ–ƒâ–„â–ˆâ–ƒâ–â–‚â–„â–ƒâ–‚â–„â–‚â–„â–‚â–„â–…â–„â–ƒâ–„
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:           valid/acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            valid/f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 16
wandb:            test/acc 0.5
wandb:             test/f1 0.33333
wandb:          train/loss 0.62461
wandb: trainer/global_step 1392
wandb:           valid/acc 0.85797
wandb:            valid/f1 0.61618
wandb: 
wandb: ğŸš€ View run upbeat-sweep-7 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/jievzz6h
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_212021-jievzz6h/logs
wandb: Agent Starting Run: mpgn90qm with config:
wandb: 	batch_size: 64
wandb: 	lr: 4.9491907145439954e-05
wandb: 	model_name: camembert-base
wandb: 	patience: 30
wandb: 	weight_decay: 0.07040331838196359
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_220143-mpgn90qm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-8
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/mpgn90qm
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                               | Params
-------------------------------------------------------------
0 | model | CamembertForSequenceClassification | 110 M 
-------------------------------------------------------------
110 M     Trainable params
0         Non-trainable params
110 M     Total params
442.497   Total estimated model params size (MB)
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.028 MB uploadedwandb: | 0.015 MB of 0.028 MB uploadedwandb: / 0.029 MB of 0.029 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:             test/f1 â–
wandb:          train/loss â–‡â–ˆâ–…â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–‚â–‚â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:           valid/acc â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–ˆâ–†â–…â–ˆâ–ˆâ–‡â–„â–…â–â–…â–†â–ˆâ–ƒâ–…â–ƒâ–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb:            valid/f1 â–‡â–‡â–‡â–‡â–‡â–…â–‡â–…â–‡â–‡â–‡â–‡â–â–…â–…â–‡â–‡â–‡â–ƒâ–ˆâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–…â–‡â–‡â–‡â–‡â–…â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb: 
wandb: Run summary:
wandb:               epoch 59
wandb:            test/acc 0.5
wandb:             test/f1 0.33333
wandb:          train/loss 0.0246
wandb: trainer/global_step 1298
wandb:           valid/acc 0.84638
wandb:            valid/f1 0.41248
wandb: 
wandb: ğŸš€ View run revived-sweep-8 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/mpgn90qm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_220143-mpgn90qm/logs
wandb: Agent Starting Run: keqipe6s with config:
wandb: 	batch_size: 64
wandb: 	lr: 0.00011318941780619688
wandb: 	model_name: camembert-base
wandb: 	patience: 30
wandb: 	weight_decay: 0.05219046438268712
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_224438-keqipe6s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sweep-9
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/keqipe6s
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                               | Params
-------------------------------------------------------------
0 | model | CamembertForSequenceClassification | 110 M 
-------------------------------------------------------------
110 M     Trainable params
0         Non-trainable params
110 M     Total params
442.497   Total estimated model params size (MB)
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.026 MB of 0.026 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            test/acc â–
wandb:             test/f1 â–
wandb:          train/loss â–†â–†â–ƒâ–‡â–„â–ƒâ–ˆâ–ƒâ–‡â–‡â–â–„â–„
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:           valid/acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            valid/f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 31
wandb:            test/acc 0.5
wandb:             test/f1 0.33333
wandb:          train/loss 0.42242
wandb: trainer/global_step 682
wandb:           valid/acc 0.85797
wandb:            valid/f1 0.41468
wandb: 
wandb: ğŸš€ View run elated-sweep-9 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/keqipe6s
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_224438-keqipe6s/logs
wandb: Agent Starting Run: sivob710 with config:
wandb: 	batch_size: 8
wandb: 	lr: 0.0008264241493869081
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 10
wandb: 	weight_decay: 0.021187556596414605
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_230715-sivob710
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-10
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/sivob710
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                               | Params
-------------------------------------------------------------
0 | model | CamembertForSequenceClassification | 336 M 
-------------------------------------------------------------
336 M     Trainable params
0         Non-trainable params
336 M     Total params
1,346.658 Total estimated model params size (MB)
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.024 MB uploadedwandb: | 0.015 MB of 0.024 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:            test/acc â–
wandb:             test/f1 â–
wandb:          train/loss â–â–ƒâ–…â–„â–„â–ƒâ–„â–ƒâ–‡â–„â–„â–…â–ƒâ–‚â–†â–â–†â–ˆâ–ƒâ–„â–„â–ƒâ–„â–‚â–ƒâ–„â–†â–â–â–‚â–„â–„â–†â–„â–„â–â–„â–†
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:           valid/acc â–â–â–â–â–â–â–â–â–â–â–
wandb:            valid/f1 â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 11
wandb:            test/acc 0.5
wandb:             test/f1 0.33333
wandb:          train/loss 0.81565
wandb: trainer/global_step 1903
wandb:           valid/acc 0.85797
wandb:            valid/f1 0.69942
wandb: 
wandb: ğŸš€ View run apricot-sweep-10 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/sivob710
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_230715-sivob710/logs
wandb: Agent Starting Run: 0adppna2 with config:
wandb: 	batch_size: 64
wandb: 	lr: 0.0006976991895766948
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 10
wandb: 	weight_decay: 0.022322042226279273
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_233740-0adppna2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sweep-11
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/0adppna2
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                               | Params
-------------------------------------------------------------
0 | model | CamembertForSequenceClassification | 336 M 
-------------------------------------------------------------
336 M     Trainable params
0         Non-trainable params
336 M     Total params
1,346.658 Total estimated model params size (MB)
wandb: WARNING Config item 'model_name' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.021 MB of 0.021 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run gentle-sweep-11 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/0adppna2
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_233740-0adppna2/logs
Run 0adppna2 errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 19, in get_test_f1
    model.train_model(batch_size=batch_size, patience=patience, max_epochs=100, test=True, ratio=[0.8, 0.2], wandb = True)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 175, in train_model
    camembert_trainer.fit(self, train_dataloaders=train_dl, val_dataloaders=val_dl)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
    self.fit_loop.run()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
    self._optimizer_step(batch_idx, closure)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
    call._call_lightning_module_hook(
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 164, in step
    loss = closure()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 318, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 57, in training_step
    out = self.forward(batch)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 51, in forward
    return self.model(
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 1063, in forward
    outputs = self.roberta(
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 888, in forward
    encoder_outputs = self.encoder(
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 538, in forward
    layer_outputs = layer_module(
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 427, in forward
    self_attention_outputs = self.attention(
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 354, in forward
    self_outputs = self.self(
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 274, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 174.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 172.94 MiB is free. Including non-PyTorch memory, this process has 19.31 GiB memory in use. Of the allocated memory 18.73 GiB is allocated by PyTorch, and 430.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 0adppna2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 19, in get_test_f1
wandb: ERROR     model.train_model(batch_size=batch_size, patience=patience, max_epochs=100, test=True, ratio=[0.8, 0.2], wandb = True)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 175, in train_model
wandb: ERROR     camembert_trainer.fit(self, train_dataloaders=train_dl, val_dataloaders=val_dl)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
wandb: ERROR     call._call_and_handle_interrupt(
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
wandb: ERROR     return trainer_fn(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
wandb: ERROR     self._run(model, ckpt_path=ckpt_path)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 987, in _run
wandb: ERROR     results = self._run_stage()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
wandb: ERROR     self.fit_loop.run()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
wandb: ERROR     self.advance()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
wandb: ERROR     self.epoch_loop.run(self._data_fetcher)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
wandb: ERROR     self.advance(data_fetcher)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
wandb: ERROR     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
wandb: ERROR     self._optimizer_step(batch_idx, closure)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
wandb: ERROR     call._call_lightning_module_hook(
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
wandb: ERROR     output = fn(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1303, in optimizer_step
wandb: ERROR     optimizer.step(closure=optimizer_closure)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 152, in step
wandb: ERROR     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
wandb: ERROR     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
wandb: ERROR     return optimizer.step(closure=closure, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
wandb: ERROR     out = func(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
wandb: ERROR     ret = func(self, *args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/optim/adamw.py", line 164, in step
wandb: ERROR     loss = closure()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
wandb: ERROR     closure_result = closure()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
wandb: ERROR     self._result = self.closure(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
wandb: ERROR     return func(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
wandb: ERROR     step_output = self._step_fn()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 318, in _training_step
wandb: ERROR     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
wandb: ERROR     output = fn(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 391, in training_step
wandb: ERROR     return self.lightning_module.training_step(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 57, in training_step
wandb: ERROR     out = self.forward(batch)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 51, in forward
wandb: ERROR     return self.model(
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 1063, in forward
wandb: ERROR     outputs = self.roberta(
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 888, in forward
wandb: ERROR     encoder_outputs = self.encoder(
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 538, in forward
wandb: ERROR     layer_outputs = layer_module(
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 427, in forward
wandb: ERROR     self_attention_outputs = self.attention(
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 354, in forward
wandb: ERROR     self_outputs = self.self(
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 274, in forward
wandb: ERROR     attention_scores = attention_scores / math.sqrt(self.attention_head_size)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 174.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 172.94 MiB is free. Including non-PyTorch memory, this process has 19.31 GiB memory in use. Of the allocated memory 18.73 GiB is allocated by PyTorch, and 430.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: m43u9mec with config:
wandb: 	batch_size: 16
wandb: 	lr: 0.0006503091462349964
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 5
wandb: 	weight_decay: 0.07158786989968925
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_233756-m43u9mec
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-12
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/m43u9mec
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run confused-sweep-12 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/m43u9mec
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_233756-m43u9mec/logs
Run m43u9mec errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run m43u9mec errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 3 more times]
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: kvws93su with config:
wandb: 	batch_size: 32
wandb: 	lr: 0.00034558642332669954
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 30
wandb: 	weight_decay: 0.06563542554874624
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_233806-kvws93su
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-13
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/kvws93su
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run likely-sweep-13 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/kvws93su
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_233806-kvws93su/logs
Run kvws93su errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run kvws93su errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 56zkmj6e with config:
wandb: 	batch_size: 64
wandb: 	lr: 0.0007140607860711947
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 5
wandb: 	weight_decay: 0.027835503446194065
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_233816-56zkmj6e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-14
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/56zkmj6e
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run legendary-sweep-14 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/56zkmj6e
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_233816-56zkmj6e/logs
Run 56zkmj6e errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 56zkmj6e errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: f3ic3kv7 with config:
wandb: 	batch_size: 64
wandb: 	lr: 0.0007340800910942008
wandb: 	model_name: camembert-base
wandb: 	patience: 15
wandb: 	weight_decay: 0.0017972128376292586
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_233826-f3ic3kv7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-sweep-15
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/f3ic3kv7
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.017 MB uploadedwandb: | 0.015 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run scarlet-sweep-15 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/f3ic3kv7
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_233826-f3ic3kv7/logs
Run f3ic3kv7 errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run f3ic3kv7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 4t87nctt with config:
wandb: 	batch_size: 8
wandb: 	lr: 0.0004400303078021826
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 5
wandb: 	weight_decay: 0.0183026208475546
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_233837-4t87nctt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-16
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/4t87nctt
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run autumn-sweep-16 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/4t87nctt
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_233837-4t87nctt/logs
Run 4t87nctt errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 4t87nctt errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: a3c2ugir with config:
wandb: 	batch_size: 32
wandb: 	lr: 0.0008520874726702049
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 5
wandb: 	weight_decay: 0.05818255184048098
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_233848-a3c2ugir
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-17
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/a3c2ugir
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run generous-sweep-17 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/a3c2ugir
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_233848-a3c2ugir/logs
Run a3c2ugir errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run a3c2ugir errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7sd9twsb with config:
wandb: 	batch_size: 64
wandb: 	lr: 2.3310666758296052e-05
wandb: 	model_name: camembert-base
wandb: 	patience: 5
wandb: 	weight_decay: 0.08160851196402119
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_233858-7sd9twsb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-18
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/7sd9twsb
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.017 MB uploadedwandb: | 0.015 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run iconic-sweep-18 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/7sd9twsb
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_233858-7sd9twsb/logs
Run 7sd9twsb errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7sd9twsb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 92ykrssc with config:
wandb: 	batch_size: 16
wandb: 	lr: 0.00020387422853205132
wandb: 	model_name: camembert-base
wandb: 	patience: 10
wandb: 	weight_decay: 0.0716611979760156
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_233909-92ykrssc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-19
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/92ykrssc
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.017 MB uploadedwandb: | 0.015 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run prime-sweep-19 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/92ykrssc
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_233909-92ykrssc/logs
Run 92ykrssc errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 92ykrssc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 4q268ho3 with config:
wandb: 	batch_size: 16
wandb: 	lr: 0.0006303824483621659
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 5
wandb: 	weight_decay: 0.07834307911414432
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_233919-4q268ho3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-20
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/4q268ho3
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.017 MB uploadedwandb: | 0.015 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run serene-sweep-20 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/4q268ho3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_233919-4q268ho3/logs
Run 4q268ho3 errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 4q268ho3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: l6hz9o9l with config:
wandb: 	batch_size: 64
wandb: 	lr: 8.752665412064455e-05
wandb: 	model_name: camembert-base
wandb: 	patience: 30
wandb: 	weight_decay: 0.028643825024945058
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_233929-l6hz9o9l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-21
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/l6hz9o9l
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run true-sweep-21 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/l6hz9o9l
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_233929-l6hz9o9l/logs
Run l6hz9o9l errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run l6hz9o9l errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 34mvlhxq with config:
wandb: 	batch_size: 16
wandb: 	lr: 0.00032260988166590127
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 10
wandb: 	weight_decay: 0.06756248625916698
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_233940-34mvlhxq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-22
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/34mvlhxq
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.017 MB uploadedwandb: | 0.015 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run dulcet-sweep-22 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/34mvlhxq
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_233940-34mvlhxq/logs
Run 34mvlhxq errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 34mvlhxq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: wd4afdfc with config:
wandb: 	batch_size: 16
wandb: 	lr: 0.00032726568142139417
wandb: 	model_name: camembert-base
wandb: 	patience: 5
wandb: 	weight_decay: 0.008529678377533356
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_233950-wd4afdfc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-23
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/wd4afdfc
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.017 MB uploadedwandb: | 0.015 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run dulcet-sweep-23 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/wd4afdfc
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_233950-wd4afdfc/logs
Run wd4afdfc errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run wd4afdfc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: v2s7vj1n with config:
wandb: 	batch_size: 64
wandb: 	lr: 0.00029995129467069156
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 30
wandb: 	weight_decay: 0.07559645604555164
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_234001-v2s7vj1n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-24
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/v2s7vj1n
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.017 MB uploadedwandb: | 0.015 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run dazzling-sweep-24 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/v2s7vj1n
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_234001-v2s7vj1n/logs
Run v2s7vj1n errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run v2s7vj1n errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 18cabpcx with config:
wandb: 	batch_size: 64
wandb: 	lr: 0.00015168576823344968
wandb: 	model_name: camembert-base
wandb: 	patience: 30
wandb: 	weight_decay: 0.06316750442631122
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_234011-18cabpcx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-25
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/18cabpcx
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.017 MB uploadedwandb: | 0.015 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run playful-sweep-25 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/18cabpcx
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_234011-18cabpcx/logs
Run 18cabpcx errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 18cabpcx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: shxvjzwg with config:
wandb: 	batch_size: 32
wandb: 	lr: 0.000946565284036546
wandb: 	model_name: camembert-base
wandb: 	patience: 30
wandb: 	weight_decay: 0.06584873805519667
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_234022-shxvjzwg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-26
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/shxvjzwg
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.017 MB uploadedwandb: | 0.015 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run fanciful-sweep-26 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/shxvjzwg
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_234022-shxvjzwg/logs
Run shxvjzwg errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run shxvjzwg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: n9gyuit0 with config:
wandb: 	batch_size: 32
wandb: 	lr: 0.0009020074015648296
wandb: 	model_name: camembert-base
wandb: 	patience: 10
wandb: 	weight_decay: 0.0034989701939724285
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_234032-n9gyuit0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-27
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/n9gyuit0
Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.017 MB uploadedwandb: | 0.015 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run deep-sweep-27 at: https://wandb.ai/balthazar-martin123/camembert_arg/runs/n9gyuit0
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240406_234032-n9gyuit0/logs
Run n9gyuit0 errored:
Traceback (most recent call last):
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
    self._function()
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
    model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
    ).to(device)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
    return super().to(*args, **kwargs)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run n9gyuit0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 308, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/./CamemBERT/sweepper.py", line 18, in get_test_f1
wandb: ERROR     model = Model(model_name, 3, lr=lr, weight_decay=weight_decay, typ="arg")
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/CamemBERT/models.py", line 43, in __init__
wandb: ERROR     ).to(device)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2556, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/raid/home/automatants/martin_bal/PoleProj/Argument-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
wandb: ERROR     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
wandb: ERROR torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 4.94 MiB is free. Including non-PyTorch memory, this process has 19.48 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 183.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: xmo9bim0 with config:
wandb: 	batch_size: 32
wandb: 	lr: 0.0008552318645261545
wandb: 	model_name: camembert/camembert-large
wandb: 	patience: 5
wandb: 	weight_decay: 0.007377132338515968
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /raid/home/automatants/martin_bal/PoleProj/Argument-detection/wandb/run-20240406_234042-xmo9bim0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-28
wandb: â­ï¸ View project at https://wandb.ai/balthazar-martin123/camembert_arg
wandb: ğŸ§¹ View sweep at https://wandb.ai/balthazar-martin123/camembert_arg/sweeps/1jw8bo35
wandb: ğŸš€ View run at https://wandb.ai/balthazar-martin123/camembert_arg/runs/xmo9bim0
/var/spool/slurm/d/job07549/slurm_script: line 24: 618984 Killed                  python './CamemBERT/sweepper.py'
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=7549.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
